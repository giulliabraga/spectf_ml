{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parzen Window Classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A brief overview \n",
    "\n",
    "Parzen window estimation, or kernel density estimation, refers to a nonparametric technique for estimating probability density functions from sample patterns.\n",
    "\n",
    "The Parzen or kernel estimate of the density at \\( x \\) is given by\n",
    "\n",
    "$$\n",
    "\\hat{p}(x) = \\frac{1}{n h} \\sum_{i=1}^{n} K\\left(\\frac{x - x_i}{h}\\right),\n",
    "$$\n",
    "\n",
    "where \n",
    "\n",
    "- $K$ is a window or a Kernel function with $K(x) > 0$ and\n",
    "$\\int_{-\\infty}^{\\infty} K(x) \\, dx = 1$\n",
    "- $h$ is the window width or smoothing parameter\n",
    "- $n$ represents the number of data examples\n",
    "\n",
    "In classifiers based on Parzen-window estimation, the densities are estimated for each category using the above equation, and a test point is assigned the class corresponding to the maximum posterior.\n",
    "\n",
    "The bandwith $h$ and the Kernel function are the most important hyperparameters to these types of classifiers, with the bandwidth being the most critical choice. If $h$ is too small, the estimate would be noisy, and if it is too large, the result is oversmoothed.\n",
    "\n",
    "Reference: [\"Classifier Design with Parzen Windows\" from Jain & Ramaswami, 1988](https://www.sciencedirect.com/science/article/abs/pii/B9780444871374500217#fn1).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Application to this project\n",
    "\n",
    "For this project, the multivariate product estimation method was adopted, described by the equation\n",
    "\n",
    "$$\n",
    "  P(x, w_k) = \\frac{1}{N}\\frac{1}{h^p} \n",
    "    \\sum \\limits _{i=1}^N \n",
    "        \\displaystyle \n",
    "          \\prod \\limits _{j=1}^p\n",
    "          K_j{\n",
    "            \\Bigg \\{\n",
    "              \\frac{x_{j} - x_{ij}}{h}\n",
    "            \\Bigg \\}\n",
    "          }.\n",
    "$$\n",
    "\n",
    "This method is more robust because it does not assume the independence of the variables. Additionaly, the gaussian kernel was chosen because the entire SPECTF dataset contains only continuous variables. The equation then becomes\n",
    "\n",
    "$$\n",
    "  P(x, w_k) = \\frac{1}{N}\\frac{1}{h^p} \n",
    "    \\sum \\limits _{i=1}^N \n",
    "        \\displaystyle \n",
    "          \\prod \\limits _{j=1}^p\n",
    "          \\frac{1}{ (2\\pi)^{\\frac{1}{2}}}\n",
    "          exp{\n",
    "            \\Bigg \\{\n",
    "            -\\frac{1}{2}\n",
    "              \\Bigg \\{\n",
    "                \\frac{x_{j} - x_{ij}}{h}\n",
    "              \\Bigg \\}^2\n",
    "            \\Bigg \\} \n",
    "          },\n",
    "$$\n",
    "\n",
    "which corresponds to the final probability density function estimator for this classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "import numpy as np\n",
    "\n",
    "class ParzenClassifier(ClassifierMixin, BaseEstimator):\n",
    "    \"\"\"\n",
    "    Parzen-based classifier using Gaussian kernel density estimation for classification problems.\n",
    "\n",
    "    Parameters\n",
    "    bandwidth (float): The smoothing parameter or window width for the Parzen window.\n",
    "    n_class (int): The number of unique classes in the dataset.\n",
    "    **params (dict): Additional parameters.\n",
    "\n",
    "    Attributes\n",
    "    means (np.array): List containing the mean of features for each class.\n",
    "    x_class (list of np.array): List of feature arrays, one for each class, containing training examples.\n",
    "    bandwidth (float): The window width for the Parzen estimator.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, bandwidth=1.0, n_class=2, **params):\n",
    "        super().__init__()\n",
    "        self.n_class = n_class\n",
    "        self.means = []\n",
    "        self.x_class = []\n",
    "        self.bandwidth = bandwidth \n",
    "\n",
    "    def fit(self, X: np.array, y: np.array):\n",
    "        \"\"\"\n",
    "        Selects the data corresponding to each class and stores it.\n",
    "\n",
    "        Parameters\n",
    "        X (np.array): The training data with shape (n_samples, n_features)\n",
    "        y (np.array):  Class labels for each sample in X\n",
    "\n",
    "        Returns\n",
    "        self (object): Fitted estimator\n",
    "        \"\"\"\n",
    "        for c in range(self.n_class):\n",
    "            _X = X[y == c]\n",
    "            if _X.shape[0] > 0:\n",
    "                self.means.append(_X.mean(0))\n",
    "                self.x_class.append(_X)\n",
    "        return self\n",
    "\n",
    "    def prod_class(self, c, h):\n",
    "        \"\"\"\n",
    "        This method is the core of the classifier, computing the probability density estimate for each class according to the stipulated equation.\n",
    "\n",
    "        Parameters\n",
    "        c (int): Class index.\n",
    "        h (float): Bandwidth for the Gaussian kernel.\n",
    "\n",
    "        Returns\n",
    "        prod (function): A function that takes data X and returns Parzen densities for class c.\n",
    "        \"\"\"\n",
    "        def prod(X):\n",
    "            n, p = self.x_class[c].shape  # Number of samples (n) and features (p)\n",
    "            dif = np.abs(X - self.x_class[c]) / h  # Compute the differences scaled by h\n",
    "            dif_gaussian = np.exp(-0.5 * (dif ** 2)) / ((2 * np.pi) ** (1 / 2))  # Gaussian kernel formula\n",
    "            dif_parzen = np.prod(dif_gaussian, 1)  # Product over all features\n",
    "            return np.sum(dif_parzen) / (n * (h ** p))  # Normalize by N and h^p\n",
    "        \n",
    "        return prod\n",
    "\n",
    "    def prod_windows_parzen(self, X):\n",
    "        \"\"\"\n",
    "        Computes the Parzen density for all classes over input data X.\n",
    "\n",
    "        Parameters\n",
    "        X (np.array): The input data.\n",
    "\n",
    "        Returns\n",
    "        parzen_estimates (np.array): Density scores for each class and each sample.\n",
    "        \"\"\"\n",
    "\n",
    "        h = self.bandwidth\n",
    "\n",
    "        parzen_estimates = np.array([np.apply_along_axis(self.prod_class(i, h), axis=1, arr=X) for i in range(self.n_class)]).T\n",
    "        \n",
    "        return parzen_estimates\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predicts the class labels for the provided data.\n",
    "\n",
    "        Parameters\n",
    "        X (np.array): Input data.\n",
    "\n",
    "        Returns\n",
    "        predicted_labels (np.array): Predicted class labels for each sample.\n",
    "        \"\"\"\n",
    "\n",
    "        predicted_labels = self.prod_windows_parzen(X).argmax(1)\n",
    "        \n",
    "        return predicted_labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A simple example of usage can be found below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.10      0.45      0.16        11\n",
      "           1       0.65      0.20      0.30        56\n",
      "\n",
      "    accuracy                           0.24        67\n",
      "   macro avg       0.37      0.33      0.23        67\n",
      "weighted avg       0.56      0.24      0.28        67\n",
      "\n",
      "[[ 5  6]\n",
      " [45 11]]\n"
     ]
    }
   ],
   "source": [
    "# Usage example\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "dataset = pd.read_csv('../data/preprocessed/SPECTF_preprocessed.csv')\n",
    "\n",
    "# dataset.sample(frac=0.1) # Uncomment this if you want a subsample\n",
    "\n",
    "# Get the features and target variable\n",
    "X = dataset.drop(columns='target')\n",
    "y = dataset['target']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y)\n",
    "\n",
    "parzen_cl = ParzenClassifier(bandwidth=0.9)\n",
    "\n",
    "parzen_cl.fit(X_train,y_train)\n",
    "\n",
    "y_pred = parzen_cl.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test,y_pred))\n",
    "\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
